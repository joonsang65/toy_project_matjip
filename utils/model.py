import pandas as pd
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.schema import Document
import re

class RestaurantRecommender:
    def __init__(self, csv_path):
        self.csv_path = csv_path
        self.vectordb = None
        self.retriever = None
        self.load_data()
        
    def load_data(self):
        """ë°ì´í„° ë¡œë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        print("ğŸ”„ ë°ì´í„° ë¡œë”© ì¤‘...")
        df = pd.read_csv(self.csv_path)
        df = df.fillna('')
        print(f"ğŸ“Š ì´ {len(df)}ê°œ ìŒì‹ì  ë°ì´í„° ë¡œë“œë¨")
        
        documents = self.create_documents(df)
        self.vectordb = self.create_vectorstore(documents)
        self.retriever = self.vectordb.as_retriever(search_kwargs={"k": 20})
        print("âœ… ë²¡í„° ì €ì¥ì†Œ ì¤€ë¹„ ì™„ë£Œ!")
        
    def create_documents(self, df):
        """ë¬¸ì„œ ìƒì„±"""
        documents = []
        for _, row in df.iterrows():
            # ê²€ìƒ‰ ìµœì í™”ëœ í…ìŠ¤íŠ¸ êµ¬ì„±
            ì—…ì†Œëª… = str(row['ì—…ì†Œëª…'])
            ì—…ì¢… = str(row['ì—…ì¢…']) 
            ì£¼ì†Œ = str(row['ë„ë¡œëª…ì£¼ì†Œ'])
            ì „í™”ë²ˆí˜¸ = str(row['ì „í™”ë²ˆí˜¸'])
            
            # ì§€ì—­ ì •ë³´ ì¶”ì¶œ
            ì§€ì—­_í‚¤ì›Œë“œ = self.extract_location_keywords(ì£¼ì†Œ)
            
            text = f"""
            ì—…ì†Œëª…: {ì—…ì†Œëª…}
            ì—…ì¢…: {ì—…ì¢…} ìŒì‹ì  ì‹ë‹¹ ë ˆìŠ¤í† ë‘
            ì£¼ì†Œ: {ì£¼ì†Œ}
            ì§€ì—­: {ì§€ì—­_í‚¤ì›Œë“œ}
            ì „í™”ë²ˆí˜¸: {ì „í™”ë²ˆí˜¸}
            
            {ì—…ì†Œëª…} {ì—…ì¢…} {ì£¼ì†Œ} {ì§€ì—­_í‚¤ì›Œë“œ}
            """.strip()
            
            metadata = {
                "ì—…ì†Œëª…": ì—…ì†Œëª…,
                "ì—…ì¢…": ì—…ì¢…,
                "ì£¼ì†Œ": ì£¼ì†Œ,
                "ì „í™”ë²ˆí˜¸": ì „í™”ë²ˆí˜¸,
                "ì§€ì—­": ì§€ì—­_í‚¤ì›Œë“œ
            }
            documents.append(Document(page_content=text, metadata=metadata))
        return documents
    
    def extract_location_keywords(self, address):
        """ì£¼ì†Œì—ì„œ ì§€ì—­ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # êµ¬ ì •ë³´
        êµ¬_pattern = r'(ì¥ì•ˆêµ¬|ê¶Œì„ êµ¬|íŒ”ë‹¬êµ¬|ì˜í†µêµ¬)'
        êµ¬_match = re.search(êµ¬_pattern, address)
        if êµ¬_match:
            keywords.append(êµ¬_match.group(1))
            
        # ë™ ì •ë³´  
        ë™_pattern = r'([ê°€-í£]+ë™)'
        ë™_matches = re.findall(ë™_pattern, address)
        keywords.extend(ë™_matches)
        
        # ë¡œ ì •ë³´
        ë¡œ_pattern = r'([ê°€-í£]+ë¡œ)'
        ë¡œ_matches = re.findall(ë¡œ_pattern, address)
        keywords.extend(ë¡œ_matches[:2])  # ìƒìœ„ 2ê°œë§Œ
        
        return ' '.join(set(keywords))
    
    def create_vectorstore(self, documents):
        """ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        embedding_model = HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask"
        )
        return FAISS.from_documents(documents, embedding_model)
    
    def expand_query(self, query):
        """ì¿¼ë¦¬ í™•ì¥"""
        # ì—…ì¢… ë§¤í•‘
        cuisine_mapping = {
            "í•œì‹": ["í•œì‹", "í•œêµ­ìŒì‹", "í•œêµ­ìš”ë¦¬", "ì •ì‹", "ë°±ë°˜", "êµ­ë°¥", "ì°Œê°œ", "ê°ˆë¹„", "ë¶ˆê³ ê¸°"],
            "ì¤‘ì‹": ["ì¤‘ì‹", "ì¤‘êµ­ìŒì‹", "ì¤‘êµ­ìš”ë¦¬", "ì§œì¥ë©´", "ì§¬ë½•", "íƒ•ìˆ˜ìœ¡", "ë§ˆë¼íƒ•"],
            "ì¼ì‹": ["ì¼ì‹", "ì¼ë³¸ìŒì‹", "ì¼ë³¸ìš”ë¦¬", "ì´ˆë°¥", "ë¼ë©˜", "ëˆê¹ŒìŠ¤", "ìš°ë™", "ìŠ¤ì‹œ"],
            "ì–‘ì‹": ["ì–‘ì‹", "ì„œì–‘ìŒì‹", "íŒŒìŠ¤íƒ€", "ìŠ¤í…Œì´í¬", "í”¼ì", "í–„ë²„ê±°"],
            "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸", "ìŒë£Œ", "ë² ì´ì»¤ë¦¬", "ë¹µì§‘"],
            "ì¹˜í‚¨": ["ì¹˜í‚¨", "ë‹­", "í”„ë¼ì´ë“œ", "ì–‘ë…ì¹˜í‚¨", "í›„ë¼ì´ë“œ"],
            "ë¶„ì‹": ["ë¶„ì‹", "ë–¡ë³¶ì´", "ìˆœëŒ€", "íŠ€ê¹€", "ê¹€ë°¥"],
            "ìˆ ì§‘": ["ìˆ ì§‘", "í¬ì¥ë§ˆì°¨", "ë§¥ì£¼", "ì†Œì£¼", "ì•ˆì£¼"]
        }
        
        # ì§€ì—­ ë§¤í•‘
        location_mapping = {
            "ì¥ì•ˆ": ["ì¥ì•ˆêµ¬", "ì¥ì•ˆë™", "ì •ìë™", "íŒŒì¥ë™", "ì˜í™”ë™"],
            "ê¶Œì„ ": ["ê¶Œì„ êµ¬", "ê¶Œì„ ë™", "êµ¬ìš´ë™", "ê¸ˆê³¡ë™"],
            "íŒ”ë‹¬": ["íŒ”ë‹¬êµ¬", "íŒ”ë‹¬ë™", "í–‰ê¶ë™", "ë§¤êµë™"],
            "ì˜í†µ": ["ì˜í†µêµ¬", "ì˜í†µë™", "ë§¤íƒ„ë™", "ì›ì²œë™"]
        }
        
        expanded_terms = [query]
        
        # ì—…ì¢… í™•ì¥
        for cuisine, terms in cuisine_mapping.items():
            if any(term in query for term in terms):
                expanded_terms.extend(terms[:3])  # ìƒìœ„ 3ê°œë§Œ
        
        # ì§€ì—­ í™•ì¥  
        for region, areas in location_mapping.items():
            if region in query:
                expanded_terms.extend(areas)
                
        return " ".join(set(expanded_terms))
    
    def filter_by_category(self, docs, query):
        """ì—…ì¢…ë³„ í•„í„°ë§"""
        category_keywords = {
            "í•œì‹": ["í•œì‹"],
            "ì¤‘ì‹": ["ì¤‘ì‹", "ì¤‘êµ­"],
            "ì¼ì‹": ["ì¼ì‹", "ì¼ë³¸"],
            "ì–‘ì‹": ["ì–‘ì‹", "ì„œì–‘"],
            "ì¹´í˜": ["ì¹´í˜", "ì»¤í”¼"],
            "ì¹˜í‚¨": ["ì¹˜í‚¨"],
            "í”¼ì": ["í”¼ì"],
            "ë¶„ì‹": ["ë¶„ì‹"]
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in query for keyword in keywords):
                filtered = [doc for doc in docs 
                          if any(kw in doc.metadata.get("ì—…ì¢…", "") for kw in keywords)]
                if filtered:
                    return filtered[:10]  # ìµœëŒ€ 10ê°œ
        
        return docs[:10]
    
    def filter_by_location(self, docs, query):
        """ì§€ì—­ë³„ í•„í„°ë§"""
        location_keywords = ["êµ¬", "ë™", "ë¡œ", "ê¸¸", "ì¥ì•ˆ", "ê¶Œì„ ", "íŒ”ë‹¬", "ì˜í†µ"]
        
        query_locations = []
        for keyword in location_keywords:
            if keyword in query:
                # ë” êµ¬ì²´ì ì¸ ì§€ì—­ëª… ì°¾ê¸°
                if keyword == "ì¥ì•ˆ":
                    query_locations.append("ì¥ì•ˆêµ¬")
                elif keyword == "ê¶Œì„ ":
                    query_locations.append("ê¶Œì„ êµ¬")
                elif keyword == "íŒ”ë‹¬":
                    query_locations.append("íŒ”ë‹¬êµ¬")
                elif keyword == "ì˜í†µ":
                    query_locations.append("ì˜í†µêµ¬")
                else:
                    query_locations.append(keyword)
        
        if query_locations:
            filtered = []
            for doc in docs:
                address = doc.metadata.get("ì£¼ì†Œ", "")
                if any(loc in address for loc in query_locations):
                    filtered.append(doc)
            return filtered[:10] if filtered else docs[:10]
        
        return docs[:10]
    
    def create_response(self, query, docs):
        """í…œí”Œë¦¿ ê¸°ë°˜ ì‘ë‹µ ìƒì„±"""
        if not docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ìŒì‹ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¤‘ë³µ ì œê±° (ì—…ì†Œëª… ê¸°ì¤€)
        seen_names = set()
        unique_docs = []
        for doc in docs:
            name = doc.metadata.get("ì—…ì†Œëª…")
            if name not in seen_names:
                seen_names.add(name)
                unique_docs.append(doc)
        
        # ìµœëŒ€ 5ê°œ ì¶”ì²œ
        top_docs = unique_docs[:5]
        
        response_parts = [f"'{query}' ê²€ìƒ‰ ê²°ê³¼ ({len(top_docs)}ê°œ ì¶”ì²œ):\n"]
        
        for i, doc in enumerate(top_docs, 1):
            name = doc.metadata.get("ì—…ì†Œëª…", "ì •ë³´ì—†ìŒ")
            category = doc.metadata.get("ì—…ì¢…", "ì •ë³´ì—†ìŒ")
            address = doc.metadata.get("ì£¼ì†Œ", "ì •ë³´ì—†ìŒ") 
            phone = doc.metadata.get("ì „í™”ë²ˆí˜¸", "ì •ë³´ì—†ìŒ")
            
            response_parts.append(
                f"{i}. ğŸ½ï¸ {name} ({category})\n"
                f"   ğŸ“ {address}\n"
                f"   ğŸ“ {phone}\n"
            )
        
        return "\n".join(response_parts)
    
    def search(self, query):
        """ìŒì‹ì  ê²€ìƒ‰ ë° ì¶”ì²œ"""
        # ì¿¼ë¦¬ í™•ì¥
        expanded_query = self.expand_query(query)
        
        # ë²¡í„° ê²€ìƒ‰
        retrieved_docs = self.retriever.get_relevant_documents(expanded_query)
        
        if not retrieved_docs:
            return "ê´€ë ¨ëœ ìŒì‹ì ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê²€ìƒ‰ì–´ë¡œ ì‹œë„í•´ë³´ì„¸ìš”."
        
        # ì—…ì¢…ë³„ í•„í„°ë§
        filtered_docs = self.filter_by_category(retrieved_docs, query)
        
        # ì§€ì—­ë³„ í•„í„°ë§  
        final_docs = self.filter_by_location(filtered_docs, query)
        
        # ì‘ë‹µ ìƒì„±
        return self.create_response(query, final_docs)

def main():
    # ì¶”ì²œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    recommender = RestaurantRecommender("/content/drive/MyDrive/ì½”ë“œì‡/ìŠ¤í”„ë¦°íŠ¸ ë¯¸ì…˜/ë¯¸ì…˜_17/merged_data.csv")
    
    print("ğŸ½ï¸ ìˆ˜ì›ì‹œ ìŒì‹ì  ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤!")
    print("ğŸ’¡ ì˜ˆì‹œ: 'í•œì‹ ë§›ì§‘', 'ì¥ì•ˆêµ¬ ì¤‘ì‹ë‹¹', 'ì˜í†µë™ ì¹´í˜', 'ê¶Œì„ êµ¬ ì¹˜í‚¨ì§‘'")
    print("ğŸšª ì¢…ë£Œ: 'exit' ì…ë ¥\n")
    
    while True:
        query = input("ğŸ” ê²€ìƒ‰ì–´: ").strip()
        
        if query.lower() in ['exit', 'quit', 'ì¢…ë£Œ']:
            print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
            
        if not query:
            print("âš ï¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue
            
        try:
            result = recommender.search(query)
            print(f"\n{result}\n")
            print("-" * 50)
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {str(e)}")
